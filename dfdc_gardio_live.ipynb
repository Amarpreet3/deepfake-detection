{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio"
      ],
      "metadata": {
        "id": "TOLmjA_xTQTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "# !pip install tensorflow\n",
        "# !pip install facenet_pytorch\n",
        "# !pip install timm\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "oJs2TrHR4KwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##kernal and training_zoo for gradio"
      ],
      "metadata": {
        "id": "yHWVR6wxTbJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCcK4dHB89Zu"
      },
      "outputs": [],
      "source": [
        "#kernel_utils\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from albumentations.augmentations.functional import image_compression\n",
        "from facenet_pytorch.models.mtcnn import MTCNN\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)\n",
        "\n",
        "\n",
        "class VideoReader:\n",
        "    \"\"\"Helper class for reading one or more frames from a video file.\"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, insets=(0, 0)):\n",
        "        \"\"\"Creates a new VideoReader.\n",
        "\n",
        "        Arguments:\n",
        "            verbose: whether to print warnings and error messages\n",
        "            insets: amount to inset the image by, as a percentage of\n",
        "                (width, height). This lets you \"zoom in\" to an image\n",
        "                to remove unimportant content around the borders.\n",
        "                Useful for face detection, which may not work if the\n",
        "                faces are too small.\n",
        "        \"\"\"\n",
        "        self.verbose = verbose\n",
        "        self.insets = insets\n",
        "\n",
        "    def read_frames(self, path, num_frames, jitter=0, seed=None):\n",
        "        \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "            jitter: if not 0, adds small random offsets to the frame indices;\n",
        "                this is useful so we don't always land on even or odd frames\n",
        "            seed: random seed for jittering; if you set this to a fixed value,\n",
        "                you probably want to set it only on the first video\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=int)\n",
        "        if jitter > 0:\n",
        "            np.random.seed(seed)\n",
        "            jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
        "            frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
        "\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_random_frames(self, path, num_frames, seed=None):\n",
        "        \"\"\"Picks the frame indices at random.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = sorted(np.random.choice(np.arange(0, frame_count), num_frames))\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frames_at_indices(self, path, frame_idxs):\n",
        "        \"\"\"Reads frames from a video and puts them into a NumPy array.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            frame_idxs: a list of frame indices. Important: should be\n",
        "                sorted from low-to-high! If an index appears multiple\n",
        "                times, the frame is still read only once.\n",
        "\n",
        "        Returns:\n",
        "            - a NumPy array of shape (num_frames, height, width, 3)\n",
        "            - a list of the frame indices that were read\n",
        "\n",
        "        Reading stops if loading a frame fails, in which case the first\n",
        "        dimension returned may actually be less than num_frames.\n",
        "\n",
        "        Returns None if an exception is thrown for any reason, or if no\n",
        "        frames were read.\n",
        "        \"\"\"\n",
        "        assert len(frame_idxs) > 0\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frames_at_indices(self, path, capture, frame_idxs):\n",
        "        try:\n",
        "            frames = []\n",
        "            idxs_read = []\n",
        "            for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
        "                # Get the next frame, but don't decode if we're not using it.\n",
        "                ret = capture.grab()\n",
        "                if not ret:\n",
        "                    if self.verbose:\n",
        "                        print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
        "                    break\n",
        "\n",
        "                # Need to look at this frame?\n",
        "                current = len(idxs_read)\n",
        "                if frame_idx == frame_idxs[current]:\n",
        "                    ret, frame = capture.retrieve()\n",
        "                    if not ret or frame is None:\n",
        "                        if self.verbose:\n",
        "                            print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                        break\n",
        "\n",
        "                    frame = self._postprocess_frame(frame)\n",
        "                    frames.append(frame)\n",
        "                    idxs_read.append(frame_idx)\n",
        "\n",
        "            if len(frames) > 0:\n",
        "                return np.stack(frames), idxs_read\n",
        "            if self.verbose:\n",
        "                print(\"No frames read from movie %s\" % path)\n",
        "            return None\n",
        "        except:\n",
        "            if self.verbose:\n",
        "                print(\"Exception while reading movie %s\" % path)\n",
        "            return None\n",
        "\n",
        "    def read_middle_frame(self, path):\n",
        "        \"\"\"Reads the frame from the middle of the video.\"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        result = self._read_frame_at_index(path, capture, frame_count // 2)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frame_at_index(self, path, frame_idx):\n",
        "        \"\"\"Reads a single frame from a video.\n",
        "\n",
        "        If you just want to read a single frame from the video, this is more\n",
        "        efficient than scanning through the video to find the frame. However,\n",
        "        for reading multiple frames it's not efficient.\n",
        "\n",
        "        My guess is that a \"streaming\" approach is more efficient than a\n",
        "        \"random access\" approach because, unless you happen to grab a keyframe,\n",
        "        the decoder still needs to read all the previous frames in order to\n",
        "        reconstruct the one you're asking for.\n",
        "\n",
        "        Returns a NumPy array of shape (1, H, W, 3) and the index of the frame,\n",
        "        or None if reading failed.\n",
        "        \"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frame_at_index(path, capture, frame_idx)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frame_at_index(self, path, capture, frame_idx):\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = capture.read()\n",
        "        if not ret or frame is None:\n",
        "            if self.verbose:\n",
        "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "            return None\n",
        "        else:\n",
        "            frame = self._postprocess_frame(frame)\n",
        "            return np.expand_dims(frame, axis=0), [frame_idx]\n",
        "\n",
        "    def _postprocess_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.insets[0] > 0:\n",
        "            W = frame.shape[1]\n",
        "            p = int(W * self.insets[0])\n",
        "            frame = frame[:, p:-p, :]\n",
        "\n",
        "        if self.insets[1] > 0:\n",
        "            H = frame.shape[1]\n",
        "            q = int(H * self.insets[1])\n",
        "            frame = frame[q:-q, :, :]\n",
        "\n",
        "        return frame\n",
        "\n",
        "\n",
        "class FaceExtractor:\n",
        "    def __init__(self, video_read_fn):\n",
        "        self.video_read_fn = video_read_fn\n",
        "        self.detector = MTCNN(margin=0, thresholds=[0.7, 0.8, 0.8], device=\"cuda\")\n",
        "\n",
        "    def process_videos(self, input_dir, filenames, video_idxs):\n",
        "        videos_read = []\n",
        "        frames_read = []\n",
        "        frames = []\n",
        "        results = []\n",
        "        for video_idx in video_idxs:\n",
        "            # Read the full-size frames from this video.\n",
        "            filename = filenames[video_idx]\n",
        "            video_path = os.path.join(input_dir, filename)\n",
        "            result = self.video_read_fn(video_path)\n",
        "            # Error? Then skip this video.\n",
        "            if result is None: continue\n",
        "\n",
        "            videos_read.append(video_idx)\n",
        "\n",
        "            # Keep track of the original frames (need them later).\n",
        "            my_frames, my_idxs = result\n",
        "\n",
        "            frames.append(my_frames)\n",
        "            frames_read.append(my_idxs)\n",
        "            for i, frame in enumerate(my_frames):\n",
        "                h, w = frame.shape[:2]\n",
        "                img = Image.fromarray(frame.astype(np.uint8))\n",
        "                img = img.resize(size=[s // 2 for s in img.size])\n",
        "\n",
        "                batch_boxes, probs = self.detector.detect(img, landmarks=False)\n",
        "\n",
        "                faces = []\n",
        "                scores = []\n",
        "                if batch_boxes is None:\n",
        "                    continue\n",
        "                for bbox, score in zip(batch_boxes, probs):\n",
        "                    if bbox is not None:\n",
        "                        xmin, ymin, xmax, ymax = [int(b * 2) for b in bbox]\n",
        "                        w = xmax - xmin\n",
        "                        h = ymax - ymin\n",
        "                        p_h = h // 3\n",
        "                        p_w = w // 3\n",
        "                        crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
        "                        faces.append(crop)\n",
        "                        scores.append(score)\n",
        "\n",
        "                frame_dict = {\"video_idx\": video_idx,\n",
        "                              \"frame_idx\": my_idxs[i],\n",
        "                              \"frame_w\": w,\n",
        "                              \"frame_h\": h,\n",
        "                              \"faces\": faces,\n",
        "                              \"scores\": scores}\n",
        "                results.append(frame_dict)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Convenience method for doing face extraction on a single video.\"\"\"\n",
        "        input_dir = os.path.dirname(video_path)\n",
        "        filenames = [os.path.basename(video_path)]\n",
        "        return self.process_videos(input_dir, filenames, [0])\n",
        "\n",
        "\n",
        "\n",
        "def confident_strategy(pred, t=0.8):\n",
        "    pred = np.array(pred)\n",
        "    sz = len(pred)\n",
        "    fakes = np.count_nonzero(pred > t)\n",
        "    # 11 frames are detected as fakes with high probability\n",
        "    if fakes > sz // 2.5 and fakes > 11:\n",
        "        return np.mean(pred[pred > t])\n",
        "    elif np.count_nonzero(pred < 0.2) > 0.9 * sz:\n",
        "        return np.mean(pred[pred < 0.2])\n",
        "    else:\n",
        "        return np.mean(pred)\n",
        "\n",
        "strategy = confident_strategy\n",
        "\n",
        "\n",
        "def put_to_center(img, input_size):\n",
        "    img = img[:input_size, :input_size]\n",
        "    image = np.zeros((input_size, input_size, 3), dtype=np.uint8)\n",
        "    start_w = (input_size - img.shape[1]) // 2\n",
        "    start_h = (input_size - img.shape[0]) // 2\n",
        "    image[start_h:start_h + img.shape[0], start_w: start_w + img.shape[1], :] = img\n",
        "    return image\n",
        "\n",
        "\n",
        "def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n",
        "    h, w = img.shape[:2]\n",
        "    if max(w, h) == size:\n",
        "        return img\n",
        "    if w > h:\n",
        "        scale = size / w\n",
        "        h = h * scale\n",
        "        w = size\n",
        "    else:\n",
        "        scale = size / h\n",
        "        w = w * scale\n",
        "        h = size\n",
        "    interpolation = interpolation_up if scale > 1 else interpolation_down\n",
        "    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def predict_on_video(face_extractor, video_path, batch_size, input_size, models, strategy=np.mean,\n",
        "                     apply_compression=False):\n",
        "    batch_size *= 4\n",
        "    try:\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "        if len(faces) > 0:\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = put_to_center(resized_face, input_size)\n",
        "                    if apply_compression:\n",
        "                        resized_face = image_compression(resized_face, quality=90, image_type=\".jpg\")\n",
        "                    if n + 1 < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        pass\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=\"cuda\").float()\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    preds = []\n",
        "                    for model in models:\n",
        "                        y_pred = model(x[:n].half())\n",
        "                        y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                        bpred = y_pred[:n].cpu().numpy()\n",
        "                        preds.append(strategy(bpred))\n",
        "                    return np.mean(preds)\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5\n",
        "\n",
        "\n",
        "def predict_on_video_set(face_extractor, videos, input_size, num_workers, test_dir, frames_per_video, models,\n",
        "                         strategy=np.mean,\n",
        "                         apply_compression=False):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(face_extractor=face_extractor, video_path=os.path.join(test_dir, filename),\n",
        "                                  input_size=input_size,\n",
        "                                  batch_size=frames_per_video,\n",
        "                                  models=models, strategy=strategy, apply_compression=apply_compression)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "    return list(predictions)\n",
        "#training.zoo.classifiers\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
        "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns\n",
        "from torch import nn\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
        "\n",
        "encoder_params = {\n",
        "    \"tf_efficientnet_b3_ns\": {\n",
        "        \"features\": 1536,\n",
        "        \"init_op\": partial(tf_efficientnet_b3_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b2_ns\": {\n",
        "        \"features\": 1408,\n",
        "        \"init_op\": partial(tf_efficientnet_b2_ns, pretrained=False, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.5)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns_03d\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_03d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_04d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b7_ns\": {\n",
        "        \"features\": 2560,\n",
        "        \"init_op\": partial(tf_efficientnet_b7_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns_04d\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def setup_srm_weights(input_channels: int = 3) -> torch.Tensor:\n",
        "    \"\"\"Creates the SRM kernels for noise analysis.\"\"\"\n",
        "    # note: values taken from Zhou et al., \"Learning Rich Features for Image Manipulation Detection\", CVPR2018\n",
        "    srm_kernel = torch.from_numpy(np.array([\n",
        "        [  # srm 1/2 horiz\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 1., -2., 1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/4\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 2., -4., 2., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/12\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-2., 8., -12., 8., -2.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "        ]\n",
        "    ])).float()\n",
        "    srm_kernel[0] /= 2\n",
        "    srm_kernel[1] /= 4\n",
        "    srm_kernel[2] /= 12\n",
        "    return srm_kernel.view(3, 1, 5, 5).repeat(1, input_channels, 1, 1)\n",
        "\n",
        "\n",
        "def setup_srm_layer(input_channels: int = 3) -> torch.nn.Module:\n",
        "    \"\"\"Creates a SRM convolution layer for noise analysis.\"\"\"\n",
        "    weights = setup_srm_weights(input_channels)\n",
        "    conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)\n",
        "    with torch.no_grad():\n",
        "        conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
        "    return conv\n",
        "\n",
        "\n",
        "class DeepFakeClassifierSRM(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.srm_conv = setup_srm_layer(3)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise = self.srm_conv(x)\n",
        "        x = self.encoder.forward_features(noise)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalWeightedAvgPool2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Global Weighted Average Pooling from paper \"Global Weighted Average\n",
        "    Pooling Bridges Pixel-level Localization and Image-level Classification\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, flatten=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(features, 1, kernel_size=1, bias=True)\n",
        "        self.flatten = flatten\n",
        "\n",
        "    def fscore(self, x):\n",
        "        m = self.conv(x)\n",
        "        m = m.sigmoid().exp()\n",
        "        return m\n",
        "\n",
        "    def norm(self, x: torch.Tensor):\n",
        "        return x / x.sum(dim=[2, 3], keepdim=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_x = x\n",
        "        x = self.fscore(x)\n",
        "        x = self.norm(x)\n",
        "        x = x * input_x\n",
        "        x = x.sum(dim=[2, 3], keepdim=not self.flatten)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepFakeClassifier(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.0) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeepFakeClassifierGWAP(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = GlobalWeightedAvgPool2d(encoder_params[encoder][\"features\"])\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio Live"
      ],
      "metadata": {
        "id": "vgxTlz-k1-SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gardio Live\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "# # Define the paths\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "# Load models\n",
        "def load_models(weights_dir, model_names):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(weights_dir, model) for model in model_names]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "    return models, device\n",
        "\n",
        "\n",
        "# Define processing function\n",
        "def process_video(video_input):\n",
        "    try:\n",
        "        # Check if the input is a file path or file-like object\n",
        "        if isinstance(video_input, str):\n",
        "            # If it's a file path, open the file\n",
        "            with open(video_input, 'rb') as video_file:\n",
        "                video_data = video_file.read()\n",
        "        else:\n",
        "            # Otherwise, assume it's a file-like object\n",
        "            video_data = video_input.read()\n",
        "\n",
        "        # Load models\n",
        "        models, device = load_models(weights_dir, model_names)\n",
        "        print(\"Models loaded successfully.\")\n",
        "\n",
        "        frames_per_video = 32\n",
        "        video_reader = VideoReader()\n",
        "        video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "        face_extractor = FaceExtractor(video_read_fn)\n",
        "        input_size = 380\n",
        "        strategy = confident_strategy\n",
        "        stime = time.time()\n",
        "\n",
        "        # Save the video data to a temporary file\n",
        "        with NamedTemporaryFile(delete=False, suffix='.mp4') as temp_file:\n",
        "            temp_file.write(video_data)\n",
        "            temp_video_path = temp_file.name\n",
        "        print(f\"Temporary video saved at: {temp_video_path}\")\n",
        "\n",
        "        # Process the single video provided\n",
        "        predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                           strategy=strategy, frames_per_video=frames_per_video, videos=[temp_video_path],\n",
        "                                           num_workers=6, test_dir=os.path.dirname(temp_video_path))\n",
        "        print(\"Predictions completed.\")\n",
        "\n",
        "        # Extract the video filename from the path\n",
        "        video_filename = os.path.basename(temp_video_path)\n",
        "        print(f\"Video filename: {video_filename}\")\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        os.remove(temp_video_path)\n",
        "        print(\"Temporary file removed.\")\n",
        "\n",
        "        # Handle predictions if it is a list\n",
        "        if isinstance(predictions, list):\n",
        "            # Assuming the list contains a single float value for this example\n",
        "            prediction_value = predictions[0]\n",
        "        else:\n",
        "            # Handle cases where predictions is a single float value\n",
        "            prediction_value = predictions\n",
        "\n",
        "        # Determine the label and conclusion based on the prediction value\n",
        "        label = \"REAL\" if prediction_value < 0.5 else \"FAKE\"\n",
        "        conclusion = \"The video is real\" if prediction_value < 0.5 else \"The video is fake\"\n",
        "\n",
        "        output_dict = {\"filename\": video_filename, \"label\": predictions, \"Conclusion\": conclusion}\n",
        "        return output_dict\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.Video(label=\"Upload Video\"),  # Video input\n",
        "    outputs=gr.JSON(label=\"Prediction\"),  # JSON output\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "2pCJHbJVs2Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface for gradio for video in test_dir"
      ],
      "metadata": {
        "id": "wMStExK4TtBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drive mount"
      ],
      "metadata": {
        "id": "YTWm0Do6ULx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9bEwBvZwvlN",
        "outputId": "e8595274-799a-44e3-858e-b262d200f7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function is taking input video but predicting for test_dir videos"
      ],
      "metadata": {
        "id": "UKM9OY5hXLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# # Define the paths\n",
        "test_dir = '/content/drive/MyDrive/deepfake_detection/test2'\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "def process_video(video_file):\n",
        "  if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-dir', type=str, default=test_dir, help=\"path to directory with videos\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    test_videos = sorted([x for x in os.listdir(args.test_dir) if x[-4:] == \".mp4\"])\n",
        "    print(\"Predicting {} videos\".format(len(test_videos)))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=test_videos,\n",
        "                                       num_workers=6, test_dir=args.test_dir)\n",
        "\n",
        "    output_dict = {\"filename\": test_videos, \"label\": predictions}\n",
        "    return output_dict\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.Video(),  # Video input\n",
        "    outputs=gr.JSON(label=\"Prediction\"),  # Text output\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "4rYiZY08TrmR",
        "outputId": "a76e3e8d-dc50-4bee-f515-e5b344745564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6e6bc3ab8afb050513.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6e6bc3ab8afb050513.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio for one specific video from test directory"
      ],
      "metadata": {
        "id": "R4AAi8PrT6wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is taking input video but predicting for specific video test_dir videos\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Define the paths\n",
        "test_video_path = '/content/drive/MyDrive/deepfake_detection/test2/afoovlsmtx.mp4'  # Specific video path\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "def process_video(video_file):\n",
        "  if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-video', type=str, default=test_video_path, help=\"path to video file\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    # Process the single video provided\n",
        "    test_video = args.test_video\n",
        "    print(\"Predicting video: {}\".format(test_video))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=[test_video],\n",
        "                                       num_workers=6, test_dir=os.path.dirname(test_video))\n",
        "\n",
        "    # Extract the video filename from the path\n",
        "    video_filename = os.path.basename(test_video)\n",
        "\n",
        "    output_dict = {\"filename\": video_filename, \"label\": predictions}\n",
        "    return output_dict\n",
        "# Create Gradio interface\n",
        "\n",
        "interface = gr.Interface(\n",
        "     fn=process_video,\n",
        "     inputs=gr.Video(),  # Video input\n",
        "     outputs=gr.JSON(label=\"Prediction\"),  # Text output\n",
        "     live=True\n",
        " )\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "53ab4d50-5ef2-4768-ce60-cb2da9dfa67e",
        "id": "6uNfCndCkY6w"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b0c5e2cd3652c0610e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b0c5e2cd3652c0610e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "YTWm0Do6ULx7",
        "OQqIwLJiStHv",
        "NknImrySSxzR",
        "zU9jpP8hS3cF",
        "4Pk_Ri7KS8bL",
        "-BoNYegHTAlx",
        "-XOXVtDSTE6P",
        "_8T84rV2TJJM",
        "N5zBTapDTMng",
        "TOLmjA_xTQTN",
        "yHWVR6wxTbJD",
        "wMStExK4TtBD",
        "UKM9OY5hXLEW",
        "R4AAi8PrT6wp",
        "vgxTlz-k1-SH"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}