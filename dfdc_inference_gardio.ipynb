{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Drive mount"
      ],
      "metadata": {
        "id": "YTWm0Do6ULx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R9bEwBvZwvlN",
        "outputId": "e8595274-799a-44e3-858e-b262d200f7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install"
      ],
      "metadata": {
        "id": "OQqIwLJiStHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nEJrySZW3fcK"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "# !pip install tensorflow\n",
        "# !pip install facenet_pytorch\n",
        "# !pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kernel_utils"
      ],
      "metadata": {
        "id": "NknImrySSxzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fCltQU_W0E4h"
      },
      "outputs": [],
      "source": [
        "#kernel_utils\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from albumentations.augmentations.functional import image_compression\n",
        "from facenet_pytorch.models.mtcnn import MTCNN\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)\n",
        "\n",
        "\n",
        "class VideoReader:\n",
        "    \"\"\"Helper class for reading one or more frames from a video file.\"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, insets=(0, 0)):\n",
        "        \"\"\"Creates a new VideoReader.\n",
        "\n",
        "        Arguments:\n",
        "            verbose: whether to print warnings and error messages\n",
        "            insets: amount to inset the image by, as a percentage of\n",
        "                (width, height). This lets you \"zoom in\" to an image\n",
        "                to remove unimportant content around the borders.\n",
        "                Useful for face detection, which may not work if the\n",
        "                faces are too small.\n",
        "        \"\"\"\n",
        "        self.verbose = verbose\n",
        "        self.insets = insets\n",
        "\n",
        "    def read_frames(self, path, num_frames, jitter=0, seed=None):\n",
        "        \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "            jitter: if not 0, adds small random offsets to the frame indices;\n",
        "                this is useful so we don't always land on even or odd frames\n",
        "            seed: random seed for jittering; if you set this to a fixed value,\n",
        "                you probably want to set it only on the first video\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=int)\n",
        "        if jitter > 0:\n",
        "            np.random.seed(seed)\n",
        "            jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
        "            frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
        "\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_random_frames(self, path, num_frames, seed=None):\n",
        "        \"\"\"Picks the frame indices at random.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = sorted(np.random.choice(np.arange(0, frame_count), num_frames))\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frames_at_indices(self, path, frame_idxs):\n",
        "        \"\"\"Reads frames from a video and puts them into a NumPy array.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            frame_idxs: a list of frame indices. Important: should be\n",
        "                sorted from low-to-high! If an index appears multiple\n",
        "                times, the frame is still read only once.\n",
        "\n",
        "        Returns:\n",
        "            - a NumPy array of shape (num_frames, height, width, 3)\n",
        "            - a list of the frame indices that were read\n",
        "\n",
        "        Reading stops if loading a frame fails, in which case the first\n",
        "        dimension returned may actually be less than num_frames.\n",
        "\n",
        "        Returns None if an exception is thrown for any reason, or if no\n",
        "        frames were read.\n",
        "        \"\"\"\n",
        "        assert len(frame_idxs) > 0\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frames_at_indices(self, path, capture, frame_idxs):\n",
        "        try:\n",
        "            frames = []\n",
        "            idxs_read = []\n",
        "            for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
        "                # Get the next frame, but don't decode if we're not using it.\n",
        "                ret = capture.grab()\n",
        "                if not ret:\n",
        "                    if self.verbose:\n",
        "                        print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
        "                    break\n",
        "\n",
        "                # Need to look at this frame?\n",
        "                current = len(idxs_read)\n",
        "                if frame_idx == frame_idxs[current]:\n",
        "                    ret, frame = capture.retrieve()\n",
        "                    if not ret or frame is None:\n",
        "                        if self.verbose:\n",
        "                            print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                        break\n",
        "\n",
        "                    frame = self._postprocess_frame(frame)\n",
        "                    frames.append(frame)\n",
        "                    idxs_read.append(frame_idx)\n",
        "\n",
        "            if len(frames) > 0:\n",
        "                return np.stack(frames), idxs_read\n",
        "            if self.verbose:\n",
        "                print(\"No frames read from movie %s\" % path)\n",
        "            return None\n",
        "        except:\n",
        "            if self.verbose:\n",
        "                print(\"Exception while reading movie %s\" % path)\n",
        "            return None\n",
        "\n",
        "    def read_middle_frame(self, path):\n",
        "        \"\"\"Reads the frame from the middle of the video.\"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        result = self._read_frame_at_index(path, capture, frame_count // 2)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frame_at_index(self, path, frame_idx):\n",
        "        \"\"\"Reads a single frame from a video.\n",
        "\n",
        "        If you just want to read a single frame from the video, this is more\n",
        "        efficient than scanning through the video to find the frame. However,\n",
        "        for reading multiple frames it's not efficient.\n",
        "\n",
        "        My guess is that a \"streaming\" approach is more efficient than a\n",
        "        \"random access\" approach because, unless you happen to grab a keyframe,\n",
        "        the decoder still needs to read all the previous frames in order to\n",
        "        reconstruct the one you're asking for.\n",
        "\n",
        "        Returns a NumPy array of shape (1, H, W, 3) and the index of the frame,\n",
        "        or None if reading failed.\n",
        "        \"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frame_at_index(path, capture, frame_idx)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frame_at_index(self, path, capture, frame_idx):\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = capture.read()\n",
        "        if not ret or frame is None:\n",
        "            if self.verbose:\n",
        "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "            return None\n",
        "        else:\n",
        "            frame = self._postprocess_frame(frame)\n",
        "            return np.expand_dims(frame, axis=0), [frame_idx]\n",
        "\n",
        "    def _postprocess_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.insets[0] > 0:\n",
        "            W = frame.shape[1]\n",
        "            p = int(W * self.insets[0])\n",
        "            frame = frame[:, p:-p, :]\n",
        "\n",
        "        if self.insets[1] > 0:\n",
        "            H = frame.shape[1]\n",
        "            q = int(H * self.insets[1])\n",
        "            frame = frame[q:-q, :, :]\n",
        "\n",
        "        return frame\n",
        "\n",
        "\n",
        "class FaceExtractor:\n",
        "    def __init__(self, video_read_fn):\n",
        "        self.video_read_fn = video_read_fn\n",
        "        self.detector = MTCNN(margin=0, thresholds=[0.7, 0.8, 0.8], device=\"cuda\")\n",
        "\n",
        "    def process_videos(self, input_dir, filenames, video_idxs):\n",
        "        videos_read = []\n",
        "        frames_read = []\n",
        "        frames = []\n",
        "        results = []\n",
        "        for video_idx in video_idxs:\n",
        "            # Read the full-size frames from this video.\n",
        "            filename = filenames[video_idx]\n",
        "            video_path = os.path.join(input_dir, filename)\n",
        "            result = self.video_read_fn(video_path)\n",
        "            # Error? Then skip this video.\n",
        "            if result is None: continue\n",
        "\n",
        "            videos_read.append(video_idx)\n",
        "\n",
        "            # Keep track of the original frames (need them later).\n",
        "            my_frames, my_idxs = result\n",
        "\n",
        "            frames.append(my_frames)\n",
        "            frames_read.append(my_idxs)\n",
        "            for i, frame in enumerate(my_frames):\n",
        "                h, w = frame.shape[:2]\n",
        "                img = Image.fromarray(frame.astype(np.uint8))\n",
        "                img = img.resize(size=[s // 2 for s in img.size])\n",
        "\n",
        "                batch_boxes, probs = self.detector.detect(img, landmarks=False)\n",
        "\n",
        "                faces = []\n",
        "                scores = []\n",
        "                if batch_boxes is None:\n",
        "                    continue\n",
        "                for bbox, score in zip(batch_boxes, probs):\n",
        "                    if bbox is not None:\n",
        "                        xmin, ymin, xmax, ymax = [int(b * 2) for b in bbox]\n",
        "                        w = xmax - xmin\n",
        "                        h = ymax - ymin\n",
        "                        p_h = h // 3\n",
        "                        p_w = w // 3\n",
        "                        crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
        "                        faces.append(crop)\n",
        "                        scores.append(score)\n",
        "\n",
        "                frame_dict = {\"video_idx\": video_idx,\n",
        "                              \"frame_idx\": my_idxs[i],\n",
        "                              \"frame_w\": w,\n",
        "                              \"frame_h\": h,\n",
        "                              \"faces\": faces,\n",
        "                              \"scores\": scores}\n",
        "                results.append(frame_dict)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Convenience method for doing face extraction on a single video.\"\"\"\n",
        "        input_dir = os.path.dirname(video_path)\n",
        "        filenames = [os.path.basename(video_path)]\n",
        "        return self.process_videos(input_dir, filenames, [0])\n",
        "\n",
        "\n",
        "\n",
        "def confident_strategy(pred, t=0.8):\n",
        "    pred = np.array(pred)\n",
        "    sz = len(pred)\n",
        "    fakes = np.count_nonzero(pred > t)\n",
        "    # 11 frames are detected as fakes with high probability\n",
        "    if fakes > sz // 2.5 and fakes > 11:\n",
        "        return np.mean(pred[pred > t])\n",
        "    elif np.count_nonzero(pred < 0.2) > 0.9 * sz:\n",
        "        return np.mean(pred[pred < 0.2])\n",
        "    else:\n",
        "        return np.mean(pred)\n",
        "\n",
        "strategy = confident_strategy\n",
        "\n",
        "\n",
        "def put_to_center(img, input_size):\n",
        "    img = img[:input_size, :input_size]\n",
        "    image = np.zeros((input_size, input_size, 3), dtype=np.uint8)\n",
        "    start_w = (input_size - img.shape[1]) // 2\n",
        "    start_h = (input_size - img.shape[0]) // 2\n",
        "    image[start_h:start_h + img.shape[0], start_w: start_w + img.shape[1], :] = img\n",
        "    return image\n",
        "\n",
        "\n",
        "def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n",
        "    h, w = img.shape[:2]\n",
        "    if max(w, h) == size:\n",
        "        return img\n",
        "    if w > h:\n",
        "        scale = size / w\n",
        "        h = h * scale\n",
        "        w = size\n",
        "    else:\n",
        "        scale = size / h\n",
        "        w = w * scale\n",
        "        h = size\n",
        "    interpolation = interpolation_up if scale > 1 else interpolation_down\n",
        "    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def predict_on_video(face_extractor, video_path, batch_size, input_size, models, strategy=np.mean,\n",
        "                     apply_compression=False):\n",
        "    batch_size *= 4\n",
        "    try:\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "        if len(faces) > 0:\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = put_to_center(resized_face, input_size)\n",
        "                    if apply_compression:\n",
        "                        resized_face = image_compression(resized_face, quality=90, image_type=\".jpg\")\n",
        "                    if n + 1 < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        pass\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=\"cuda\").float()\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    preds = []\n",
        "                    for model in models:\n",
        "                        y_pred = model(x[:n].half())\n",
        "                        y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                        bpred = y_pred[:n].cpu().numpy()\n",
        "                        preds.append(strategy(bpred))\n",
        "                    return np.mean(preds)\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5\n",
        "\n",
        "\n",
        "def predict_on_video_set(face_extractor, videos, input_size, num_workers, test_dir, frames_per_video, models,\n",
        "                         strategy=np.mean,\n",
        "                         apply_compression=False):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(face_extractor=face_extractor, video_path=os.path.join(test_dir, filename),\n",
        "                                  input_size=input_size,\n",
        "                                  batch_size=frames_per_video,\n",
        "                                  models=models, strategy=strategy, apply_compression=apply_compression)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "    return list(predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training.zoo.classifiers"
      ],
      "metadata": {
        "id": "zU9jpP8hS3cF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aHXkXFqh0fLJ"
      },
      "outputs": [],
      "source": [
        "#training.zoo.classifiers\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
        "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns\n",
        "from torch import nn\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
        "\n",
        "encoder_params = {\n",
        "    \"tf_efficientnet_b3_ns\": {\n",
        "        \"features\": 1536,\n",
        "        \"init_op\": partial(tf_efficientnet_b3_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b2_ns\": {\n",
        "        \"features\": 1408,\n",
        "        \"init_op\": partial(tf_efficientnet_b2_ns, pretrained=False, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.5)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns_03d\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_03d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_04d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b7_ns\": {\n",
        "        \"features\": 2560,\n",
        "        \"init_op\": partial(tf_efficientnet_b7_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns_04d\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def setup_srm_weights(input_channels: int = 3) -> torch.Tensor:\n",
        "    \"\"\"Creates the SRM kernels for noise analysis.\"\"\"\n",
        "    # note: values taken from Zhou et al., \"Learning Rich Features for Image Manipulation Detection\", CVPR2018\n",
        "    srm_kernel = torch.from_numpy(np.array([\n",
        "        [  # srm 1/2 horiz\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 1., -2., 1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/4\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 2., -4., 2., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/12\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-2., 8., -12., 8., -2.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "        ]\n",
        "    ])).float()\n",
        "    srm_kernel[0] /= 2\n",
        "    srm_kernel[1] /= 4\n",
        "    srm_kernel[2] /= 12\n",
        "    return srm_kernel.view(3, 1, 5, 5).repeat(1, input_channels, 1, 1)\n",
        "\n",
        "\n",
        "def setup_srm_layer(input_channels: int = 3) -> torch.nn.Module:\n",
        "    \"\"\"Creates a SRM convolution layer for noise analysis.\"\"\"\n",
        "    weights = setup_srm_weights(input_channels)\n",
        "    conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)\n",
        "    with torch.no_grad():\n",
        "        conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
        "    return conv\n",
        "\n",
        "\n",
        "class DeepFakeClassifierSRM(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.srm_conv = setup_srm_layer(3)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise = self.srm_conv(x)\n",
        "        x = self.encoder.forward_features(noise)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalWeightedAvgPool2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Global Weighted Average Pooling from paper \"Global Weighted Average\n",
        "    Pooling Bridges Pixel-level Localization and Image-level Classification\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, flatten=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(features, 1, kernel_size=1, bias=True)\n",
        "        self.flatten = flatten\n",
        "\n",
        "    def fscore(self, x):\n",
        "        m = self.conv(x)\n",
        "        m = m.sigmoid().exp()\n",
        "        return m\n",
        "\n",
        "    def norm(self, x: torch.Tensor):\n",
        "        return x / x.sum(dim=[2, 3], keepdim=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_x = x\n",
        "        x = self.fscore(x)\n",
        "        x = self.norm(x)\n",
        "        x = x * input_x\n",
        "        x = x.sum(dim=[2, 3], keepdim=not self.flatten)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepFakeClassifier(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.0) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeepFakeClassifierGWAP(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = GlobalWeightedAvgPool2d(encoder_params[encoder][\"features\"])\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Videos"
      ],
      "metadata": {
        "id": "4Pk_Ri7KS8bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path\n",
        "test_dir = '/content/drive/MyDrive/deepfake_detection/test2'\n",
        "\n",
        "# List all files and directories in the specified path\n",
        "contents = os.listdir(test_dir)\n",
        "\n",
        "# Print the contents\n",
        "for item in contents:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "StwUQL4lBzaF",
        "outputId": "94e6676e-419a-4ddd-d4ac-566f9ef6c674"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agqphdxmwt.mp4\n",
            "aagfhgtpmv.mp4\n",
            "afoovlsmtx.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference for all videos in test directory"
      ],
      "metadata": {
        "id": "-BoNYegHTAlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "d43764fd6a354da2b643a882e0ff478b",
            "8976538bdb0840399aaa084055a44d34",
            "a14c6741400e48788a6634326d58ec2e",
            "9012d9c9c7724fad837b8dfe161559b3",
            "66507eecf86044c385312f523ba02ef6",
            "721e478cd7c648b8ad10db6b6b0ff722",
            "a88f2cc54e85448d8bdd41a57da6a274",
            "2bd63f58cf62453b808682418b9f3607",
            "b655eddb86e447d2bd036616bde7496a",
            "8bbbeb9a84bb489fbf119f22be152a18",
            "1d3d993a2c1a4305a4af6339a033e78f"
          ]
        },
        "id": "ezI5g4wMZ2Nv",
        "outputId": "37a9f444-8bce-4199-9c47-701361731695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ea7b308709b6>:145: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
            "  self.encoder = encoder_params[encoder][\"init_op\"]()\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/267M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d43764fd6a354da2b643a882e0ff478b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23\n",
            "Predicting 3 videos\n",
            "Elapsed: 26.184617519378662\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the paths\n",
        "test_dir = '/content/drive/MyDrive/deepfake_detection/test2'\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-dir', type=str, default=test_dir, help=\"path to directory with videos\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    test_videos = sorted([x for x in os.listdir(args.test_dir) if x[-4:] == \".mp4\"])\n",
        "    print(\"Predicting {} videos\".format(len(test_videos)))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=test_videos,\n",
        "                                       num_workers=6, test_dir=args.test_dir)\n",
        "    submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
        "    submission_df.to_csv(args.output, index=False)\n",
        "    print(\"Elapsed:\", time.time() - stime)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output"
      ],
      "metadata": {
        "id": "-XOXVtDSTE6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RBj60h2XAgwe",
        "outputId": "6705ea86-b352-4ab8-9258-b947e3f617b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         filename    label\n",
            "0  aagfhgtpmv.mp4  0.98900\n",
            "1  afoovlsmtx.mp4  0.00888\n",
            "2  agqphdxmwt.mp4  0.98900\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the output file\n",
        "output_path = '/content/submission.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "submission_df = pd.read_csv(output_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(submission_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For one video only"
      ],
      "metadata": {
        "id": "_8T84rV2TJJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# Define the paths\n",
        "test_video_path = '/content/drive/MyDrive/deepfake_detection/test2/afoovlsmtx.mp4'  # Specific video path\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-video', type=str, default=test_video_path, help=\"path to video file\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    # Process the specific video\n",
        "    test_video = args.test_video\n",
        "    print(\"Predicting video: {}\".format(test_video))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=[test_video],\n",
        "                                       num_workers=6, test_dir=os.path.dirname(test_video))\n",
        "\n",
        "    # Extract the video filename from the path\n",
        "    video_filename = os.path.basename(test_video)\n",
        "\n",
        "    # Create the DataFrame and save to CSV\n",
        "    submission_df = pd.DataFrame({\"filename\": [video_filename], \"label\": predictions})\n",
        "    submission_df.to_csv(args.output, index=False)\n",
        "    print(\"Elapsed:\", time.time() - stime)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u5_MFB8MdKS5",
        "outputId": "4a4a96c9-44ef-4f51-f1c1-fc7430cbc753"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-ea7b308709b6>:145: UserWarning: Mapping deprecated model name tf_efficientnet_b7_ns to current tf_efficientnet_b7.ns_jft_in1k.\n",
            "  self.encoder = encoder_params[encoder][\"init_op\"]()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40\n",
            "loading state dict /content/drive/MyDrive/deepfake_detection/dfdc_models/final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23\n",
            "Predicting video: /content/drive/MyDrive/deepfake_detection/test2/afoovlsmtx.mp4\n",
            "Elapsed: 7.989045143127441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#output for one video"
      ],
      "metadata": {
        "id": "N5zBTapDTMng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the output file\n",
        "output_path = '/content/submission.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "submission_df = pd.read_csv(output_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p8SET_3LJTTF",
        "outputId": "06f94942-f0f3-4718-b86b-d16c820ab81c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         filename    label\n",
            "0  afoovlsmtx.mp4  0.00888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio"
      ],
      "metadata": {
        "id": "TOLmjA_xTQTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "# !pip install tensorflow\n",
        "# !pip install facenet_pytorch\n",
        "# !pip install timm\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "oJs2TrHR4KwY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kernal and training_zoo for gradio"
      ],
      "metadata": {
        "id": "yHWVR6wxTbJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qCcK4dHB89Zu"
      },
      "outputs": [],
      "source": [
        "#kernel_utils\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from albumentations.augmentations.functional import image_compression\n",
        "from facenet_pytorch.models.mtcnn import MTCNN\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)\n",
        "\n",
        "\n",
        "class VideoReader:\n",
        "    \"\"\"Helper class for reading one or more frames from a video file.\"\"\"\n",
        "\n",
        "    def __init__(self, verbose=True, insets=(0, 0)):\n",
        "        \"\"\"Creates a new VideoReader.\n",
        "\n",
        "        Arguments:\n",
        "            verbose: whether to print warnings and error messages\n",
        "            insets: amount to inset the image by, as a percentage of\n",
        "                (width, height). This lets you \"zoom in\" to an image\n",
        "                to remove unimportant content around the borders.\n",
        "                Useful for face detection, which may not work if the\n",
        "                faces are too small.\n",
        "        \"\"\"\n",
        "        self.verbose = verbose\n",
        "        self.insets = insets\n",
        "\n",
        "    def read_frames(self, path, num_frames, jitter=0, seed=None):\n",
        "        \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "            jitter: if not 0, adds small random offsets to the frame indices;\n",
        "                this is useful so we don't always land on even or odd frames\n",
        "            seed: random seed for jittering; if you set this to a fixed value,\n",
        "                you probably want to set it only on the first video\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=int)\n",
        "        if jitter > 0:\n",
        "            np.random.seed(seed)\n",
        "            jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
        "            frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
        "\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_random_frames(self, path, num_frames, seed=None):\n",
        "        \"\"\"Picks the frame indices at random.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            num_frames: how many frames to read, -1 means the entire video\n",
        "                (warning: this will take up a lot of memory!)\n",
        "        \"\"\"\n",
        "        assert num_frames > 0\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if frame_count <= 0: return None\n",
        "\n",
        "        frame_idxs = sorted(np.random.choice(np.arange(0, frame_count), num_frames))\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frames_at_indices(self, path, frame_idxs):\n",
        "        \"\"\"Reads frames from a video and puts them into a NumPy array.\n",
        "\n",
        "        Arguments:\n",
        "            path: the video file\n",
        "            frame_idxs: a list of frame indices. Important: should be\n",
        "                sorted from low-to-high! If an index appears multiple\n",
        "                times, the frame is still read only once.\n",
        "\n",
        "        Returns:\n",
        "            - a NumPy array of shape (num_frames, height, width, 3)\n",
        "            - a list of the frame indices that were read\n",
        "\n",
        "        Reading stops if loading a frame fails, in which case the first\n",
        "        dimension returned may actually be less than num_frames.\n",
        "\n",
        "        Returns None if an exception is thrown for any reason, or if no\n",
        "        frames were read.\n",
        "        \"\"\"\n",
        "        assert len(frame_idxs) > 0\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frames_at_indices(self, path, capture, frame_idxs):\n",
        "        try:\n",
        "            frames = []\n",
        "            idxs_read = []\n",
        "            for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
        "                # Get the next frame, but don't decode if we're not using it.\n",
        "                ret = capture.grab()\n",
        "                if not ret:\n",
        "                    if self.verbose:\n",
        "                        print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
        "                    break\n",
        "\n",
        "                # Need to look at this frame?\n",
        "                current = len(idxs_read)\n",
        "                if frame_idx == frame_idxs[current]:\n",
        "                    ret, frame = capture.retrieve()\n",
        "                    if not ret or frame is None:\n",
        "                        if self.verbose:\n",
        "                            print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                        break\n",
        "\n",
        "                    frame = self._postprocess_frame(frame)\n",
        "                    frames.append(frame)\n",
        "                    idxs_read.append(frame_idx)\n",
        "\n",
        "            if len(frames) > 0:\n",
        "                return np.stack(frames), idxs_read\n",
        "            if self.verbose:\n",
        "                print(\"No frames read from movie %s\" % path)\n",
        "            return None\n",
        "        except:\n",
        "            if self.verbose:\n",
        "                print(\"Exception while reading movie %s\" % path)\n",
        "            return None\n",
        "\n",
        "    def read_middle_frame(self, path):\n",
        "        \"\"\"Reads the frame from the middle of the video.\"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        result = self._read_frame_at_index(path, capture, frame_count // 2)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def read_frame_at_index(self, path, frame_idx):\n",
        "        \"\"\"Reads a single frame from a video.\n",
        "\n",
        "        If you just want to read a single frame from the video, this is more\n",
        "        efficient than scanning through the video to find the frame. However,\n",
        "        for reading multiple frames it's not efficient.\n",
        "\n",
        "        My guess is that a \"streaming\" approach is more efficient than a\n",
        "        \"random access\" approach because, unless you happen to grab a keyframe,\n",
        "        the decoder still needs to read all the previous frames in order to\n",
        "        reconstruct the one you're asking for.\n",
        "\n",
        "        Returns a NumPy array of shape (1, H, W, 3) and the index of the frame,\n",
        "        or None if reading failed.\n",
        "        \"\"\"\n",
        "        capture = cv2.VideoCapture(path)\n",
        "        result = self._read_frame_at_index(path, capture, frame_idx)\n",
        "        capture.release()\n",
        "        return result\n",
        "\n",
        "    def _read_frame_at_index(self, path, capture, frame_idx):\n",
        "        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = capture.read()\n",
        "        if not ret or frame is None:\n",
        "            if self.verbose:\n",
        "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "            return None\n",
        "        else:\n",
        "            frame = self._postprocess_frame(frame)\n",
        "            return np.expand_dims(frame, axis=0), [frame_idx]\n",
        "\n",
        "    def _postprocess_frame(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.insets[0] > 0:\n",
        "            W = frame.shape[1]\n",
        "            p = int(W * self.insets[0])\n",
        "            frame = frame[:, p:-p, :]\n",
        "\n",
        "        if self.insets[1] > 0:\n",
        "            H = frame.shape[1]\n",
        "            q = int(H * self.insets[1])\n",
        "            frame = frame[q:-q, :, :]\n",
        "\n",
        "        return frame\n",
        "\n",
        "\n",
        "class FaceExtractor:\n",
        "    def __init__(self, video_read_fn):\n",
        "        self.video_read_fn = video_read_fn\n",
        "        self.detector = MTCNN(margin=0, thresholds=[0.7, 0.8, 0.8], device=\"cuda\")\n",
        "\n",
        "    def process_videos(self, input_dir, filenames, video_idxs):\n",
        "        videos_read = []\n",
        "        frames_read = []\n",
        "        frames = []\n",
        "        results = []\n",
        "        for video_idx in video_idxs:\n",
        "            # Read the full-size frames from this video.\n",
        "            filename = filenames[video_idx]\n",
        "            video_path = os.path.join(input_dir, filename)\n",
        "            result = self.video_read_fn(video_path)\n",
        "            # Error? Then skip this video.\n",
        "            if result is None: continue\n",
        "\n",
        "            videos_read.append(video_idx)\n",
        "\n",
        "            # Keep track of the original frames (need them later).\n",
        "            my_frames, my_idxs = result\n",
        "\n",
        "            frames.append(my_frames)\n",
        "            frames_read.append(my_idxs)\n",
        "            for i, frame in enumerate(my_frames):\n",
        "                h, w = frame.shape[:2]\n",
        "                img = Image.fromarray(frame.astype(np.uint8))\n",
        "                img = img.resize(size=[s // 2 for s in img.size])\n",
        "\n",
        "                batch_boxes, probs = self.detector.detect(img, landmarks=False)\n",
        "\n",
        "                faces = []\n",
        "                scores = []\n",
        "                if batch_boxes is None:\n",
        "                    continue\n",
        "                for bbox, score in zip(batch_boxes, probs):\n",
        "                    if bbox is not None:\n",
        "                        xmin, ymin, xmax, ymax = [int(b * 2) for b in bbox]\n",
        "                        w = xmax - xmin\n",
        "                        h = ymax - ymin\n",
        "                        p_h = h // 3\n",
        "                        p_w = w // 3\n",
        "                        crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
        "                        faces.append(crop)\n",
        "                        scores.append(score)\n",
        "\n",
        "                frame_dict = {\"video_idx\": video_idx,\n",
        "                              \"frame_idx\": my_idxs[i],\n",
        "                              \"frame_w\": w,\n",
        "                              \"frame_h\": h,\n",
        "                              \"faces\": faces,\n",
        "                              \"scores\": scores}\n",
        "                results.append(frame_dict)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Convenience method for doing face extraction on a single video.\"\"\"\n",
        "        input_dir = os.path.dirname(video_path)\n",
        "        filenames = [os.path.basename(video_path)]\n",
        "        return self.process_videos(input_dir, filenames, [0])\n",
        "\n",
        "\n",
        "\n",
        "def confident_strategy(pred, t=0.8):\n",
        "    pred = np.array(pred)\n",
        "    sz = len(pred)\n",
        "    fakes = np.count_nonzero(pred > t)\n",
        "    # 11 frames are detected as fakes with high probability\n",
        "    if fakes > sz // 2.5 and fakes > 11:\n",
        "        return np.mean(pred[pred > t])\n",
        "    elif np.count_nonzero(pred < 0.2) > 0.9 * sz:\n",
        "        return np.mean(pred[pred < 0.2])\n",
        "    else:\n",
        "        return np.mean(pred)\n",
        "\n",
        "strategy = confident_strategy\n",
        "\n",
        "\n",
        "def put_to_center(img, input_size):\n",
        "    img = img[:input_size, :input_size]\n",
        "    image = np.zeros((input_size, input_size, 3), dtype=np.uint8)\n",
        "    start_w = (input_size - img.shape[1]) // 2\n",
        "    start_h = (input_size - img.shape[0]) // 2\n",
        "    image[start_h:start_h + img.shape[0], start_w: start_w + img.shape[1], :] = img\n",
        "    return image\n",
        "\n",
        "\n",
        "def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n",
        "    h, w = img.shape[:2]\n",
        "    if max(w, h) == size:\n",
        "        return img\n",
        "    if w > h:\n",
        "        scale = size / w\n",
        "        h = h * scale\n",
        "        w = size\n",
        "    else:\n",
        "        scale = size / h\n",
        "        w = w * scale\n",
        "        h = size\n",
        "    interpolation = interpolation_up if scale > 1 else interpolation_down\n",
        "    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def predict_on_video(face_extractor, video_path, batch_size, input_size, models, strategy=np.mean,\n",
        "                     apply_compression=False):\n",
        "    batch_size *= 4\n",
        "    try:\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "        if len(faces) > 0:\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = put_to_center(resized_face, input_size)\n",
        "                    if apply_compression:\n",
        "                        resized_face = image_compression(resized_face, quality=90, image_type=\".jpg\")\n",
        "                    if n + 1 < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        pass\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=\"cuda\").float()\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    preds = []\n",
        "                    for model in models:\n",
        "                        y_pred = model(x[:n].half())\n",
        "                        y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                        bpred = y_pred[:n].cpu().numpy()\n",
        "                        preds.append(strategy(bpred))\n",
        "                    return np.mean(preds)\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5\n",
        "\n",
        "\n",
        "def predict_on_video_set(face_extractor, videos, input_size, num_workers, test_dir, frames_per_video, models,\n",
        "                         strategy=np.mean,\n",
        "                         apply_compression=False):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(face_extractor=face_extractor, video_path=os.path.join(test_dir, filename),\n",
        "                                  input_size=input_size,\n",
        "                                  batch_size=frames_per_video,\n",
        "                                  models=models, strategy=strategy, apply_compression=apply_compression)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "    return list(predictions)\n",
        "#training.zoo.classifiers\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
        "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns\n",
        "from torch import nn\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.pooling import AdaptiveAvgPool2d\n",
        "\n",
        "encoder_params = {\n",
        "    \"tf_efficientnet_b3_ns\": {\n",
        "        \"features\": 1536,\n",
        "        \"init_op\": partial(tf_efficientnet_b3_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b2_ns\": {\n",
        "        \"features\": 1408,\n",
        "        \"init_op\": partial(tf_efficientnet_b2_ns, pretrained=False, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.5)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b4_ns_03d\": {\n",
        "        \"features\": 1792,\n",
        "        \"init_op\": partial(tf_efficientnet_b4_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_03d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.3)\n",
        "    },\n",
        "    \"tf_efficientnet_b5_ns_04d\": {\n",
        "        \"features\": 2048,\n",
        "        \"init_op\": partial(tf_efficientnet_b5_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b7_ns\": {\n",
        "        \"features\": 2560,\n",
        "        \"init_op\": partial(tf_efficientnet_b7_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b6_ns_04d\": {\n",
        "        \"features\": 2304,\n",
        "        \"init_op\": partial(tf_efficientnet_b6_ns, pretrained=True, drop_path_rate=0.4)\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def setup_srm_weights(input_channels: int = 3) -> torch.Tensor:\n",
        "    \"\"\"Creates the SRM kernels for noise analysis.\"\"\"\n",
        "    # note: values taken from Zhou et al., \"Learning Rich Features for Image Manipulation Detection\", CVPR2018\n",
        "    srm_kernel = torch.from_numpy(np.array([\n",
        "        [  # srm 1/2 horiz\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 1., -2., 1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/4\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 2., -4., 2., 0.],  # noqa: E241,E201\n",
        "            [0., -1., 2., -1., 0.],  # noqa: E241,E201\n",
        "            [0., 0., 0., 0., 0.],  # noqa: E241,E201\n",
        "        ], [  # srm 1/12\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-2., 8., -12., 8., -2.],  # noqa: E241,E201\n",
        "            [2., -6., 8., -6., 2.],  # noqa: E241,E201\n",
        "            [-1., 2., -2., 2., -1.],  # noqa: E241,E201\n",
        "        ]\n",
        "    ])).float()\n",
        "    srm_kernel[0] /= 2\n",
        "    srm_kernel[1] /= 4\n",
        "    srm_kernel[2] /= 12\n",
        "    return srm_kernel.view(3, 1, 5, 5).repeat(1, input_channels, 1, 1)\n",
        "\n",
        "\n",
        "def setup_srm_layer(input_channels: int = 3) -> torch.nn.Module:\n",
        "    \"\"\"Creates a SRM convolution layer for noise analysis.\"\"\"\n",
        "    weights = setup_srm_weights(input_channels)\n",
        "    conv = torch.nn.Conv2d(input_channels, out_channels=3, kernel_size=5, stride=1, padding=2, bias=False)\n",
        "    with torch.no_grad():\n",
        "        conv.weight = torch.nn.Parameter(weights, requires_grad=False)\n",
        "    return conv\n",
        "\n",
        "\n",
        "class DeepFakeClassifierSRM(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.srm_conv = setup_srm_layer(3)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        noise = self.srm_conv(x)\n",
        "        x = self.encoder.forward_features(noise)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalWeightedAvgPool2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Global Weighted Average Pooling from paper \"Global Weighted Average\n",
        "    Pooling Bridges Pixel-level Localization and Image-level Classification\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, flatten=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(features, 1, kernel_size=1, bias=True)\n",
        "        self.flatten = flatten\n",
        "\n",
        "    def fscore(self, x):\n",
        "        m = self.conv(x)\n",
        "        m = m.sigmoid().exp()\n",
        "        return m\n",
        "\n",
        "    def norm(self, x: torch.Tensor):\n",
        "        return x / x.sum(dim=[2, 3], keepdim=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_x = x\n",
        "        x = self.fscore(x)\n",
        "        x = self.norm(x)\n",
        "        x = x * input_x\n",
        "        x = x.sum(dim=[2, 3], keepdim=not self.flatten)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DeepFakeClassifier(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.0) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeepFakeClassifierGWAP(nn.Module):\n",
        "    def __init__(self, encoder, dropout_rate=0.5) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = GlobalWeightedAvgPool2d(encoder_params[encoder][\"features\"])\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface for gradio for video in test_dir"
      ],
      "metadata": {
        "id": "wMStExK4TtBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function is taking input video but predicting for test_dir videos"
      ],
      "metadata": {
        "id": "UKM9OY5hXLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# # Define the paths\n",
        "test_dir = '/content/drive/MyDrive/deepfake_detection/test2'\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "def process_video(video_file):\n",
        "  if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-dir', type=str, default=test_dir, help=\"path to directory with videos\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    test_videos = sorted([x for x in os.listdir(args.test_dir) if x[-4:] == \".mp4\"])\n",
        "    print(\"Predicting {} videos\".format(len(test_videos)))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=test_videos,\n",
        "                                       num_workers=6, test_dir=args.test_dir)\n",
        "\n",
        "    output_dict = {\"filename\": test_videos, \"label\": predictions}\n",
        "    return output_dict\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.Video(),  # Video input\n",
        "    outputs=gr.JSON(label=\"Prediction\"),  # Text output\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "4rYiZY08TrmR",
        "outputId": "a76e3e8d-dc50-4bee-f515-e5b344745564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6e6bc3ab8afb050513.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6e6bc3ab8afb050513.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio for one specific video from test directory"
      ],
      "metadata": {
        "id": "R4AAi8PrT6wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is taking input video but predicting for specific video test_dir videos\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Define the paths\n",
        "test_video_path = '/content/drive/MyDrive/deepfake_detection/test2/afoovlsmtx.mp4'  # Specific video path\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "def process_video(video_file):\n",
        "  if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"Predict test videos\")\n",
        "    arg = parser.add_argument\n",
        "    arg('--weights-dir', type=str, default=weights_dir, help=\"path to directory with checkpoints\")\n",
        "    arg('--models', nargs='+', default=model_names, help=\"checkpoint files\")\n",
        "    arg('--test-video', type=str, default=test_video_path, help=\"path to video file\")\n",
        "    arg('--output', type=str, default=output, help=\"path to output csv\")\n",
        "    args, unknown = parser.parse_known_args()  # Use parse_known_args to ignore Jupyter's extra arguments\n",
        "\n",
        "    # GPU Availability Check\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(args.weights_dir, model) for model in args.models]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        print(\"loading state dict {}\".format(path))\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "\n",
        "    frames_per_video = 32\n",
        "    video_reader = VideoReader()\n",
        "    video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "    face_extractor = FaceExtractor(video_read_fn)\n",
        "    input_size = 380\n",
        "    strategy = confident_strategy\n",
        "    stime = time.time()\n",
        "\n",
        "    # Process the single video provided\n",
        "    test_video = args.test_video\n",
        "    print(\"Predicting video: {}\".format(test_video))\n",
        "    predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                       strategy=strategy, frames_per_video=frames_per_video, videos=[test_video],\n",
        "                                       num_workers=6, test_dir=os.path.dirname(test_video))\n",
        "\n",
        "    # Extract the video filename from the path\n",
        "    video_filename = os.path.basename(test_video)\n",
        "\n",
        "    output_dict = {\"filename\": video_filename, \"label\": predictions}\n",
        "    return output_dict\n",
        "# Create Gradio interface\n",
        "\n",
        "interface = gr.Interface(\n",
        "     fn=process_video,\n",
        "     inputs=gr.Video(),  # Video input\n",
        "     outputs=gr.JSON(label=\"Prediction\"),  # Text output\n",
        "     live=True\n",
        " )\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "53ab4d50-5ef2-4768-ce60-cb2da9dfa67e",
        "id": "6uNfCndCkY6w"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b0c5e2cd3652c0610e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b0c5e2cd3652c0610e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradio Live"
      ],
      "metadata": {
        "id": "vgxTlz-k1-SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gardio Live\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "# # Define the paths\n",
        "output = '/content/submission.csv'\n",
        "weights_dir = '/content/drive/MyDrive/deepfake_detection/dfdc_models'\n",
        "model_names = [\n",
        "    'final_111_DeepFakeClassifier_tf_efficientnet_b7_ns_0_36',\n",
        "    'final_555_DeepFakeClassifier_tf_efficientnet_b7_ns_0_19',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_29',\n",
        "    'final_777_DeepFakeClassifier_tf_efficientnet_b7_ns_0_31',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_37',\n",
        "    'final_888_DeepFakeClassifier_tf_efficientnet_b7_ns_0_40',\n",
        "    'final_999_DeepFakeClassifier_tf_efficientnet_b7_ns_0_23'\n",
        "]\n",
        "\n",
        "# Load models\n",
        "def load_models(weights_dir, model_names):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    models = []\n",
        "    model_paths = [os.path.join(weights_dir, model) for model in model_names]\n",
        "    for path in model_paths:\n",
        "        model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(device)\n",
        "        checkpoint = torch.load(path, map_location=\"cpu\")\n",
        "        state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "        model.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=True)\n",
        "        model.eval()\n",
        "        del checkpoint\n",
        "        models.append(model.half())\n",
        "    return models, device\n",
        "\n",
        "\n",
        "# Define processing function\n",
        "def process_video(video_input):\n",
        "    try:\n",
        "        # Check if the input is a file path or file-like object\n",
        "        if isinstance(video_input, str):\n",
        "            # If it's a file path, open the file\n",
        "            with open(video_input, 'rb') as video_file:\n",
        "                video_data = video_file.read()\n",
        "        else:\n",
        "            # Otherwise, assume it's a file-like object\n",
        "            video_data = video_input.read()\n",
        "\n",
        "        # Load models\n",
        "        models, device = load_models(weights_dir, model_names)\n",
        "        print(\"Models loaded successfully.\")\n",
        "\n",
        "        frames_per_video = 32\n",
        "        video_reader = VideoReader()\n",
        "        video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "        face_extractor = FaceExtractor(video_read_fn)\n",
        "        input_size = 380\n",
        "        strategy = confident_strategy\n",
        "        stime = time.time()\n",
        "\n",
        "        # Save the video data to a temporary file\n",
        "        with NamedTemporaryFile(delete=False, suffix='.mp4') as temp_file:\n",
        "            temp_file.write(video_data)\n",
        "            temp_video_path = temp_file.name\n",
        "        print(f\"Temporary video saved at: {temp_video_path}\")\n",
        "\n",
        "        # Process the single video provided\n",
        "        predictions = predict_on_video_set(face_extractor=face_extractor, input_size=input_size, models=models,\n",
        "                                           strategy=strategy, frames_per_video=frames_per_video, videos=[temp_video_path],\n",
        "                                           num_workers=6, test_dir=os.path.dirname(temp_video_path))\n",
        "        print(\"Predictions completed.\")\n",
        "\n",
        "        # Extract the video filename from the path\n",
        "        video_filename = os.path.basename(temp_video_path)\n",
        "        print(f\"Video filename: {video_filename}\")\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        os.remove(temp_video_path)\n",
        "        print(\"Temporary file removed.\")\n",
        "\n",
        "        # Handle predictions if it is a list\n",
        "        if isinstance(predictions, list):\n",
        "            # Assuming the list contains a single float value for this example\n",
        "            prediction_value = predictions[0]\n",
        "        else:\n",
        "            # Handle cases where predictions is a single float value\n",
        "            prediction_value = predictions\n",
        "\n",
        "        # Determine the label and conclusion based on the prediction value\n",
        "        label = \"REAL\" if prediction_value < 0.5 else \"FAKE\"\n",
        "        conclusion = \"The video is real\" if prediction_value < 0.5 else \"The video is fake\"\n",
        "\n",
        "        output_dict = {\"filename\": video_filename, \"label\": predictions, \"Conclusion\": conclusion}\n",
        "        return output_dict\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_video,\n",
        "    inputs=gr.Video(label=\"Upload Video\"),  # Video input\n",
        "    outputs=gr.JSON(label=\"Prediction\"),  # JSON output\n",
        "    live=True\n",
        ")\n",
        "\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "2pCJHbJVs2Ty"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d43764fd6a354da2b643a882e0ff478b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8976538bdb0840399aaa084055a44d34",
              "IPY_MODEL_a14c6741400e48788a6634326d58ec2e",
              "IPY_MODEL_9012d9c9c7724fad837b8dfe161559b3"
            ],
            "layout": "IPY_MODEL_66507eecf86044c385312f523ba02ef6"
          }
        },
        "8976538bdb0840399aaa084055a44d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_721e478cd7c648b8ad10db6b6b0ff722",
            "placeholder": "​",
            "style": "IPY_MODEL_a88f2cc54e85448d8bdd41a57da6a274",
            "value": "model.safetensors: 100%"
          }
        },
        "a14c6741400e48788a6634326d58ec2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd63f58cf62453b808682418b9f3607",
            "max": 266748382,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b655eddb86e447d2bd036616bde7496a",
            "value": 266748382
          }
        },
        "9012d9c9c7724fad837b8dfe161559b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bbbeb9a84bb489fbf119f22be152a18",
            "placeholder": "​",
            "style": "IPY_MODEL_1d3d993a2c1a4305a4af6339a033e78f",
            "value": " 267M/267M [00:23&lt;00:00, 23.6MB/s]"
          }
        },
        "66507eecf86044c385312f523ba02ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721e478cd7c648b8ad10db6b6b0ff722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88f2cc54e85448d8bdd41a57da6a274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bd63f58cf62453b808682418b9f3607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b655eddb86e447d2bd036616bde7496a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bbbeb9a84bb489fbf119f22be152a18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d3d993a2c1a4305a4af6339a033e78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}